{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 5123 - Cloud Computing and Distributed Systems\n",
    "## Assignment - 3 Prerequisites - Google Collab setup for Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Upload this notebook to Google Colab to use it\n",
    "###### Let's setup Spark first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete/Comment this line when running in the cluster\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Import required libraries now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    " \n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Let's initialize Spark context now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Spark context with necessary configuration\n",
    "conf = SparkConf()\n",
    "conf.setMaster('local')\n",
    "conf.setAppName('WordCount')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Follow the tutorial to mount your Google Drive. Give mounted Drive paths below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give **.txt FILE PATHS** here\n",
    "# You can use any random input file of your choice\n",
    "file1 = ''\n",
    "\n",
    "# USE THIS FILE as input FOR ALL BELOW tasks\n",
    "# Change filepath to HDFS path when executing in the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Spark program - Wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Spark application for a simple wordcount\n",
    "# What is wordcount? \n",
    "    # Given a file, count the frequency of all words appearing in that file\n",
    "    \n",
    "# Step-1: Read the required file. In our case it is file1 or file2.\n",
    "# NOTE: We do not need to initialize SparkContext as only one SparkContext can be initialized in one notebook\n",
    "fileRDD = sc.textFile(file1)\n",
    "\n",
    "# Step-2: \n",
    "    # Each line in our file(s) is a sentence. So, we need to split the sentence with ' ' to get words\n",
    "    # Using map() will return RDD[list]. But we need RDD[string]. So we use flatMap()\n",
    "wordsRDD = fileRDD.flatMap(lambda line: line.split(\" \")) # <----------- TEST what happens when you use map()\n",
    "\n",
    "# Step-3: For each input, we will make (K,V) pair, where K is the word and V is 1\n",
    "pairRDD = wordsRDD.map(lambda word: (word,1))\n",
    "\n",
    "# Step-4: Now we have to sum all 1's of each word\n",
    "# NOTE: A word may present in multiple data partitions. So we use reduceByKey() to group by key and perform sum\n",
    "countRDD = pairRDD.reduceByKey(lambda a,b: a+b)\n",
    "\n",
    "#Step-5: Save results in a text file\n",
    "countRDD.saveAsTextFile('') # <----------- GIVE FILE PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple task"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The above example wordcount program does not include any text processing like converting upper cases to lower cases, removing non-English words, etc. \n",
    "\n",
    "Modify the above Spark program to convert upper cases and remove non-English words. Notice the difference in the generated output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Download your tasks as .py file and try to execute both the above tasks in the CS department Hadoop cluster\n",
    "Spark execution command documentation can be found at: https://spark.apache.org/docs/latest/submitting-applications.html\n",
    "\n",
    "Example execution command: spark-submit Assignment3_Prerequisites.py --master yarn --deloy-mode client --num-executors 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
